{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["This is a sample Python script."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Press Shift+F10 to execute it or replace it with your code.<br>\n", "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import numpy as np\n", "import PIL\n", "import tensorflow as tf\n", "import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "from PIL import Image, ImageFilter\n", "from tensorflow import keras\n", "from keras import layers\n", "from keras.models import Sequential"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def print_hi(name):\n", "    # Use a breakpoint in the code line below to debug your script.\n", "    print(f'Hi, {name}')  # Press Ctrl+F8 to toggle the breakpoint."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def edge_variant(img):\n", "    # Converting Original Image to\n", "    img = img.Convert(\"L\")\n", "    final = img.Filter(ImageFilter.Kernel((3, 3), (-1, -1, -1, -1, 8, -1, -1, -1, -1), 1, 0))\n", "    return final"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_model():\n", "    batch_size = 32\n", "    img_height = 640\n", "    img_width = 480\n", "    seed = 728\n", "    train_ds = tf.keras.utils.image_dataset_from_directory(\n", "        'train',\n", "        validation_split=0.2,\n", "        subset=\"training\",\n", "        seed=seed,\n", "        image_size=(img_height, img_width),\n", "        batch_size=batch_size)\n", "    val_ds = tf.keras.utils.image_dataset_from_directory(\n", "        'train',\n", "        validation_split=0.2,\n", "        subset=\"validation\",\n", "        seed=seed,\n", "        image_size=(img_height, img_width),\n", "        batch_size=batch_size)\n", "    class_names = train_ds.class_names\n", "    normalization_layer = layers.Rescaling(1. / 255)\n", "    normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n", "    image_batch, labels_batch = next(iter(normalized_ds))\n", "    num_classes = len(class_names)\n", "    data_augmentation = keras.Sequential(\n", "        [\n", "            layers.RandomFlip(\"horizontal\",\n", "                              input_shape=(img_height,\n", "                                           img_width,\n", "                                           3)),\n", "            layers.RandomRotation(0.05),\n", "        ]\n", "    )\n", "    model = Sequential([\n", "        data_augmentation,\n", "        layers.Rescaling(1. / 255),\n", "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n", "        layers.MaxPooling2D(),\n", "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n", "        layers.MaxPooling2D(),\n", "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n", "        layers.MaxPooling2D(),\n", "        layers.Dropout(0.2),\n", "        layers.Flatten(),\n", "        layers.Dense(128, activation='relu'),\n", "        layers.Dense(num_classes, name=\"outputs\")\n", "    ])\n", "    model.compile(optimizer='adam',\n", "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n", "                  metrics=['accuracy'])\n", "    model.summary()\n", "    checkpoint_path = \"training_1/cp.ckpt\"\n", "    checkpoint_dir = os.path.dirname(checkpoint_path)\n\n", "    # Create a callback that saves the model's weights\n", "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n", "                                                     save_weights_only=True,\n", "                                                     verbose=1)\n", "    epochs = 10000\n", "    history = model.fit(\n", "        train_ds,\n", "        validation_data=val_ds,\n", "        epochs=epochs,\n", "        callbacks=[cp_callback]\n", "    )\n", "    for filename in os.listdir('small'):\n", "        f = os.path.join('small', filename)\n", "        # checking if it is a file\n", "        img = tf.keras.utils.load_img(\n", "            f, target_size=(img_height, img_width)\n", "        )\n", "        img_array = tf.keras.utils.img_to_array(img)\n", "        img_array = tf.expand_dims(img_array, 0)  # Create a batch\n", "        predictions = model.predict(img_array)\n", "        score = tf.nn.softmax(predictions[0])\n", "        print(\n", "            \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n", "            .format(class_names[np.argmax(score)], 100 * np.max(score))\n", "        )\n", "    print_hi('PyCharm')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def continue_model():\n", "    batch_size = 32\n", "    img_height = 640\n", "    img_width = 480\n", "    seed = 777\n", "    train_ds = tf.keras.utils.image_dataset_from_directory(\n", "        'train',\n", "        validation_split=0.2,\n", "        subset=\"training\",\n", "        seed=seed,\n", "        image_size=(img_height, img_width),\n", "        batch_size=batch_size)\n", "    val_ds = tf.keras.utils.image_dataset_from_directory(\n", "        'train',\n", "        validation_split=0.2,\n", "        subset=\"validation\",\n", "        seed=seed,\n", "        image_size=(img_height, img_width),\n", "        batch_size=batch_size)\n", "    class_names = train_ds.class_names\n", "    normalization_layer = layers.Rescaling(1. / 255)\n", "    normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n", "    image_batch, labels_batch = next(iter(normalized_ds))\n", "    num_classes = len(class_names)\n", "    data_augmentation = keras.Sequential(\n", "        [\n", "            layers.RandomFlip(\"horizontal\",\n", "                              input_shape=(img_height,\n", "                                           img_width,\n", "                                           3)),\n", "            layers.RandomRotation(0.1),\n", "            layers.RandomZoom(0.1)\n", "        ]\n", "    )\n", "    model = Sequential([\n", "        data_augmentation,\n", "        layers.Rescaling(1. / 255),\n", "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n", "        layers.MaxPooling2D(),\n", "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n", "        layers.MaxPooling2D(),\n", "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n", "        layers.MaxPooling2D(),\n", "        layers.Dropout(0.2),\n", "        layers.Flatten(),\n", "        layers.Dense(128, activation='relu'),\n", "        layers.Dense(num_classes, name=\"outputs\")\n", "    ])\n", "    model.compile(optimizer='adam',\n", "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n", "                  metrics=['accuracy'])\n", "    model.summary()\n", "    checkpoint_path = \"training_1/cp.ckpt\"\n", "    checkpoint_dir = os.path.dirname(checkpoint_path)\n", "    model.load_weights(checkpoint_path)\n\n", "    # Create a callback that saves the model's weights\n", "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n", "                                                     save_weights_only=True,\n", "                                                     verbose=1)\n", "    epochs = 100\n", "    history = model.fit(\n", "        train_ds,\n", "        validation_data=val_ds,\n", "        epochs=epochs,\n", "        callbacks=[cp_callback]\n", "    )\n", "    csvfile = open('csvfile.csv', 'w')\n", "    csvfile.write('file_index,class\\n')\n", "    for filename in os.listdir('test'):\n", "        f = os.path.join('test', filename)\n", "        # checking if it is a file\n", "        img = tf.keras.utils.load_img(\n", "            f, target_size=(img_height, img_width)\n", "        )\n", "        img_array = tf.keras.utils.img_to_array(img)\n", "        img_array = tf.expand_dims(img_array, 0)  # Create a batch\n", "        predictions = model.predict(img_array)\n", "        score = tf.nn.softmax(predictions[0])\n", "        if int(np.argmax(score)) == 0:\n", "            csvfile.write(Path(f).stem + ',0' + '\\n')\n", "        else:\n", "            csvfile.write(Path(f).stem + ',1' + '\\n')\n", "        print(\n", "            \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n", "            .format(class_names[np.argmax(score)], 100 * np.max(score))\n", "        )\n", "    csvfile.close()\n", "    print_hi('PyCharm')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def test_model():\n", "    batch_size = 32\n", "    img_height = 640\n", "    img_width = 480\n", "    seed = 777\n", "    train_ds = tf.keras.utils.image_dataset_from_directory(\n", "        'train',\n", "        validation_split=0.2,\n", "        subset=\"training\",\n", "        seed=seed,\n", "        image_size=(img_height, img_width),\n", "        batch_size=batch_size)\n", "    val_ds = tf.keras.utils.image_dataset_from_directory(\n", "        'train',\n", "        validation_split=0.2,\n", "        subset=\"validation\",\n", "        seed=seed,\n", "        image_size=(img_height, img_width),\n", "        batch_size=batch_size)\n", "    class_names = train_ds.class_names\n", "    normalization_layer = layers.Rescaling(1. / 255)\n", "    normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n", "    image_batch, labels_batch = next(iter(normalized_ds))\n", "    num_classes = len(class_names)\n", "    data_augmentation = keras.Sequential(\n", "        [\n", "            layers.RandomFlip(\"horizontal\",\n", "                              input_shape=(img_height,\n", "                                           img_width,\n", "                                           3)),\n", "            layers.RandomRotation(0.05),\n", "        ]\n", "    )\n", "    model = Sequential([\n", "        data_augmentation,\n", "        layers.Rescaling(1. / 255),\n", "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n", "        layers.MaxPooling2D(),\n", "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n", "        layers.MaxPooling2D(),\n", "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n", "        layers.MaxPooling2D(),\n", "        layers.Dropout(0.2),\n", "        layers.Flatten(),\n", "        layers.Dense(128, activation='relu'),\n", "        layers.Dense(num_classes, name=\"outputs\")\n", "    ])\n", "    model.compile(optimizer='adam',\n", "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n", "                  metrics=['accuracy'])\n", "    model.summary()\n", "    checkpoint_path = \"training_1/cp.ckpt\"\n", "    checkpoint_dir = os.path.dirname(checkpoint_path)\n", "    model.load_weights(checkpoint_path)\n", "    csvfile = open('csvfile.csv', 'w')\n", "    csvfile.write('file_index,class\\n')\n", "    for filename in os.listdir('test'):\n", "        f = os.path.join('test', filename)\n", "        # checking if it is a file\n", "        img = tf.keras.utils.load_img(\n", "            f, target_size=(img_height, img_width)\n", "        )\n", "        img_array = tf.keras.utils.img_to_array(img)\n", "        img_array = tf.expand_dims(img_array, 0)  # Create a batch\n", "        predictions = model.predict(img_array)\n", "        score = tf.nn.softmax(predictions[0])\n", "        if int(np.argmax(score)) == 0 and 100 * np.max(score) > 50.0:\n", "            csvfile.write(Path(f).stem + ',0' + '\\n')\n", "        else:\n", "            csvfile.write(Path(f).stem + ',1' + '\\n')\n", "        print(\n", "            \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n", "            .format(class_names[np.argmax(score)], 100 * np.max(score))\n", "        )\n", "    csvfile.close()\n", "    print_hi('PyCharm')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Values<br>\n", "0 - True<br>\n", "1 - Wrong<br>\n", "2 - Screen<br>\n", "3 - Screen+Photoshop<br>\n", "4 - Photoshop<br>\n", "Press the green button in the gutter to run the script."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n", "    action = int(input('0 For building new, 1 For continuing, 2 For testing  '))\n", "    if action == 0:\n", "        build_model()\n", "    elif action == 1:\n", "        continue_model()\n", "    else:\n", "        test_model()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["See PyCharm help at https://www.jetbrains.com/help/pycharm/"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}